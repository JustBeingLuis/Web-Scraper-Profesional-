
# ğŸ•·ï¸ Web Scraper Profesional 

Este proyecto es una herramienta de **scraping profesional automatizado**, diseÃ±ada para extraer informaciÃ³n de productos desde sitios web de comercio electrÃ³nico que utilizan contenido dinÃ¡mico (renderizado con JavaScript). La aplicaciÃ³n genera automÃ¡ticamente **informes en tres formatos**: `PDF`, `Excel` y `JSON`, incluyendo el **nombre del producto, precio e imagen** de cada Ã­tem disponible.

> ğŸ” Este scraper ha sido probado exitosamente con el sitio oficial de [Tiendas D1](https://domicilios.tiendasd1.com/), especÃ­ficamente en la categorÃ­a de congelados.

---

## ğŸ“Œ CaracterÃ­sticas Principales

| Funcionalidad                        | DescripciÃ³n |
|-------------------------------------|-------------|
| ğŸ§  Scraping inteligente              | Usa `Selenium` para procesar contenido dinÃ¡mico. |
| ğŸ“¦ ExtracciÃ³n de datos estructurados| Nombre, precio e imagen del producto. |
| ğŸ§¾ ExportaciÃ³n en formatos mÃºltiples | JSON, Excel (`.xlsx`) y PDF con imÃ¡genes integradas. |
| ğŸ› ï¸ CÃ³digo modular y extensible       | SeparaciÃ³n clara de funciones por archivo. |
| ğŸ’¡ Adaptable a mÃºltiples pÃ¡ginas    | Soporte para ajustar selectores y estructuras HTML. |

---

## ğŸš€ Â¿CÃ³mo funciona?

1. El usuario ingresa la URL de la pÃ¡gina que desea scrapear.
2. Se lanza un navegador automatizado (como Edge o Brave) mediante Selenium.
3. Se espera a que cargue el contenido dinÃ¡mico (productos).
4. Se extrae la informaciÃ³n relevante utilizando `BeautifulSoup`.
5. Se generan 3 tipos de informes automÃ¡ticamente:
   - ğŸ“„ `informe.pdf` con texto e imÃ¡genes.
   - ğŸ“Š `informe.xlsx` listo para anÃ¡lisis en Excel o Power BI.
   - ğŸ§¾ `informe.json` con la estructura de los datos.

---

## ğŸ“ Estructura del proyecto

```
web-scraper-app/
â”œâ”€â”€ main.py              # Script principal (ejecuta todo)
â”œâ”€â”€ scraper.py           # LÃ³gica de scraping y parsing HTML
â”œâ”€â”€ reports.py           # GeneraciÃ³n de informes PDF, Excel y JSON
â”œâ”€â”€ requirements.txt     # Dependencias del proyecto
â”œâ”€â”€ informe.xlsx         # Informe generado en Excel
â”œâ”€â”€ informe.pdf          # Informe PDF con imÃ¡genes
â”œâ”€â”€ informe.json         # Datos en formato estructurado
```

---

## ğŸ”§ Â¿CÃ³mo adaptarlo a otras pÃ¡ginas?

Este scraper **es totalmente adaptable a otros sitios web**, pero es necesario comprender cÃ³mo funciona la estructura del HTML en cada caso. AquÃ­ estÃ¡n los pasos para adaptarlo:

### 1. ğŸ” Analiza el HTML de la nueva pÃ¡gina

- Abre el navegador, haz clic derecho sobre el producto y selecciona **"Inspeccionar"**.
- Identifica el **contenedor principal** de cada producto (ej. `div.product-card`, `li.item`, etc.).
- Dentro de ese contenedor, ubica:
  - **Nombre del producto**: Â¿estÃ¡ en un `<p>`, `<h2>`, `<span>`?
  - **Precio**: Â¿es un `<p>`, `<div>`?
  - **Imagen**: busca la etiqueta `<img>` y asegÃºrate de extraer el atributo `src`.

### 2. âœï¸ Ajusta los selectores CSS

Los selectores actuales funcionan solo para Tiendas D1. Para otros sitios debes modificar:

- La clase del **contenedor del producto**
- La clase y tipo de etiqueta donde se encuentra el **nombre**
- La clase y tipo de etiqueta donde estÃ¡ el **precio**
- La ubicaciÃ³n de la **imagen** (`img`) si existe

> âš ï¸ **Importante:** El selector debe incluir tanto la **etiqueta HTML** como su clase, por ejemplo:  
> `p.nombre-producto`, `div.precio`, `img.product-image`.

### 3. âœ… Verifica que los datos estÃ©n bien extraÃ­dos

Puedes imprimir temporalmente los productos extraÃ­dos en consola con:

```python
print(productos)
```

y revisar si estÃ¡n correctos antes de generar los informes.

---

## ğŸ§ª Requisitos del entorno

- Python 3.8 o superior
- Navegador **Microsoft Edge** (recomendado por compatibilidad con Selenium)
- Google Chrome o Brave (tambiÃ©n soportados, con ajustes)
- Paquetes requeridos:
  - `selenium`
  - `bs4`
  - `requests`
  - `fpdf`
  - `pandas`
  - `openpyxl`
  - `webdriver-manager`
  - `Pillow`

---

## âš™ï¸ Uso del proyecto

```bash
# Clonar el repositorio
git clone https://github.com/tuusuario/web-scraper-app.git
cd web-scraper-app

# Instalar dependencias
pip install -r requirements.txt

# Ejecutar el scraper
python main.py
```

## ğŸ‘¨â€ğŸ’» Autor

**Luis Toscano**  
Estudiante de IngenierÃ­a de Sistemas â€“ UIS  
ğŸ”¬ Proyecto desarrollado con fines investigativos y de portafolio.  
Presentado en el marco de prÃ¡cticas de scraping profesional.

---

## ğŸ“„ Licencia

Este proyecto es de libre uso educativo. No se recomienda utilizarlo para scraping masivo sin autorizaciÃ³n de las pÃ¡ginas web involucradas.
